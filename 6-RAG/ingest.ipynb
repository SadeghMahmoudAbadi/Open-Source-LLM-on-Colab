{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwuiyKyv4Smhs0qEn9XoSi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SadeghMahmoudAbadi/Open-Source-LLM-on-Colab/blob/main/6-RAG/ingest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3hmHpEyr1nl",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install litellm\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field\n",
        "from chromadb import PersistentClient\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool\n",
        "from tenacity import retry, wait_exponential\n",
        "import pickle\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "2ia6YfPI9e6F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gemini-2.5-flash\"\n",
        "\n",
        "DB_NAME = \"/content/drive/MyDrive/datasets/preprocessed_db\"\n",
        "collection_name = \"docs\"\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "KNOWLEDGE_BASE_PATH = Path(\"/content/drive/MyDrive/datasets/knowledge-base\")\n",
        "AVERAGE_CHUNK_SIZE = 200\n",
        "wait = wait_exponential(multiplier=1, min=10, max=240)\n",
        "google_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "chunks_path = '/content/drive/MyDrive/datasets/chunk_list.pkl'\n",
        "\n",
        "WORKERS = 3"
      ],
      "metadata": {
        "id": "eqHJWFG3-nit"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "e2Szdv_aKjhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini = OpenAI(\n",
        "    api_key=google_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")"
      ],
      "metadata": {
        "id": "wy5Bsf8wDr0M"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = gemini.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Tell me a light-hearted joke.\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "Z1trtz3yr66e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Result(BaseModel):\n",
        "    page_content: str\n",
        "    metadata: dict"
      ],
      "metadata": {
        "id": "p8P0jHYELsXr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Chunk(BaseModel):\n",
        "    headline: str = Field(\n",
        "        description=\"A brief heading for this chunk, typically a few words, that is most likely to be surfaced in a query\",\n",
        "    )\n",
        "    summary: str = Field(\n",
        "        description=\"A few sentences summarizing the content of this chunk to answer common questions\"\n",
        "    )\n",
        "    original_text: str = Field(\n",
        "        description=\"The original text of this chunk from the provided document, exactly as is, not changed in any way\"\n",
        "    )\n",
        "\n",
        "    def as_result(self, document):\n",
        "        metadata = {\"source\": document[\"source\"], \"type\": document[\"type\"]}\n",
        "        return Result(\n",
        "            page_content=self.headline + \"\\n\\n\" + self.summary + \"\\n\\n\" + self.original_text,\n",
        "            metadata=metadata,\n",
        "        )\n",
        "\n",
        "\n",
        "class Chunks(BaseModel):\n",
        "    chunks: list[Chunk]"
      ],
      "metadata": {
        "id": "3CCuE4W4LuVp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_documents():\n",
        "    \"\"\"A homemade version of the LangChain DirectoryLoader\"\"\"\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    for folder in KNOWLEDGE_BASE_PATH.iterdir():\n",
        "        doc_type = folder.name\n",
        "        for file in folder.rglob(\"*.md\"):\n",
        "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "                documents.append({\"type\": doc_type, \"source\": file.as_posix(), \"text\": f.read()})\n",
        "\n",
        "    print(f\"Loaded {len(documents)} documents\")\n",
        "    return documents"
      ],
      "metadata": {
        "id": "L2V5hJBoLxEq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = fetch_documents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hS9oWZhL1Bt",
        "outputId": "d8b2d9d9-158b-4ed8-c6c6-3265a35d8d7c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 76 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(document):\n",
        "    how_many = (len(document[\"text\"]) // AVERAGE_CHUNK_SIZE) + 1\n",
        "    return f\"\"\"\n",
        "    You take a document and you split the document into overlapping chunks for a KnowledgeBase.\n",
        "\n",
        "    The document is from the shared drive of a company called Insurellm.\n",
        "    The document is of type: {document[\"type\"]}\n",
        "    The document has been retrieved from: {document[\"source\"]}\n",
        "\n",
        "    A chatbot will use these chunks to answer questions about the company.\n",
        "    You should divide up the document as you see fit, being sure that the entire document is returned across the chunks - don't leave anything out.\n",
        "    This document should probably be split into at least {how_many} chunks, but you can have more or less as appropriate, ensuring that there are individual chunks to answer specific questions.\n",
        "    There should be overlap between the chunks as appropriate; typically about 25% overlap or about 50 words, so you have the same text in multiple chunks for best retrieval results.\n",
        "\n",
        "    For each chunk, you should provide a headline, a summary, and the original text of the chunk.\n",
        "    Together your chunks should represent the entire document with overlap.\n",
        "\n",
        "    Here is the document:\n",
        "\n",
        "    {document[\"text\"]}\n",
        "\n",
        "    Respond with the chunks.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "a0CrC98YL2Wo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(make_prompt(documents[0]))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "licLqgKzMJG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_messages(document):\n",
        "    return [\n",
        "        {\"role\": \"user\", \"content\": make_prompt(document)},\n",
        "    ]"
      ],
      "metadata": {
        "id": "RqXzfiLhMLFk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_messages(documents[0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "USlOPe3zMNVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait)\n",
        "def process_document(document):\n",
        "    messages = make_messages(document)\n",
        "    response = gemini.chat.completions.parse(model=MODEL, messages=messages, response_format=Chunks)\n",
        "    reply = response.choices[0].message.content\n",
        "    doc_as_chunks = Chunks.model_validate_json(reply).chunks\n",
        "    return [chunk.as_result(document) for chunk in doc_as_chunks]"
      ],
      "metadata": {
        "id": "-bpFiqeaMOn8"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_document(documents[0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jaz5dQdxMWp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_chunks(documents):\n",
        "    \"\"\"\n",
        "    Create chunks using a number of workers in parallel.\n",
        "    If you get a rate limit error, set the WORKERS to 1.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    with Pool(processes=WORKERS) as pool:\n",
        "        for result in tqdm(pool.imap_unordered(process_document, documents), total=len(documents)):\n",
        "            chunks.extend(result)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "bZ3P4s8BMXsa"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = create_chunks(documents)"
      ],
      "metadata": {
        "id": "WBPVebzo_ow4",
        "outputId": "489bee23-e1e0-4ea1-f04e-fb6fae371628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [18:04<00:00, 14.27s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(chunks_path, 'wb') as f:\n",
        "    pickle.dump(chunks, f)"
      ],
      "metadata": {
        "id": "0jFWPWuyHGzP"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(chunks_path, 'rb') as f:\n",
        "    chunks = pickle.load(f)\n",
        "\n",
        "print(len(chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld2ynnKJZFY7",
        "outputId": "27bacfc7-5b6e-46a0-b140-506f8e0aab61"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings(chunks):\n",
        "    chroma = PersistentClient(path=DB_NAME)\n",
        "    if collection_name in [c.name for c in chroma.list_collections()]:\n",
        "        chroma.delete_collection(collection_name)\n",
        "\n",
        "    texts = [chunk.page_content for chunk in chunks]\n",
        "    emb = SentenceTransformer(embedding_model).encode(texts)\n",
        "    vectors = [e for e in emb]\n",
        "\n",
        "    collection = chroma.get_or_create_collection(collection_name)\n",
        "\n",
        "    ids = [str(i) for i in range(len(chunks))]\n",
        "    metas = [chunk.metadata for chunk in chunks]\n",
        "\n",
        "    collection.add(ids=ids, embeddings=vectors, documents=texts, metadatas=metas)\n",
        "    print(f\"Vectorstore created with {collection.count()} documents\")"
      ],
      "metadata": {
        "id": "d3ZEUJrbmwsL"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_embeddings(chunks)"
      ],
      "metadata": {
        "id": "ooniOKjQmwbl",
        "outputId": "cf0bbbd8-8009-4ac7-b4f1-d82ebe7a1d1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorstore created with 3636 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0pwyoPJvXPAL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}