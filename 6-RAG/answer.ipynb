{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP58Ema4f3fGxotZzTZ2sT9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SadeghMahmoudAbadi/Open-Source-LLM-on-Colab/blob/main/6-RAG/answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install litellm\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C6Ac5_Uqsdb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jCOiCJZRrwEv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "from chromadb import PersistentClient\n",
        "from litellm import completion\n",
        "from pydantic import BaseModel, Field\n",
        "from pathlib import Path\n",
        "from tenacity import retry, wait_exponential\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENROUTER_API_KEY\"] = userdata.get(\"OPENROUTER_API_KEY\")"
      ],
      "metadata": {
        "id": "ipc6AhfQu_2A"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_jiIYQbsb1h",
        "outputId": "4145909b-3627-45fc-eddb-34f7706cfc63"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL=\"openrouter/x-ai/grok-4.1-fast\"\n",
        "DB_NAME = \"/content/drive/MyDrive/datasets/preprocessed_db\"\n",
        "KNOWLEDGE_BASE_PATH = Path(\"/content/drive/MyDrive/datasets/knowledge-base\")\n",
        "\n",
        "collection_name = \"docs\"\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "wait = wait_exponential(multiplier=1, min=10, max=240)\n",
        "\n",
        "chroma = PersistentClient(path=DB_NAME)\n",
        "collection = chroma.get_or_create_collection(collection_name)\n",
        "\n",
        "RETRIEVAL_K = 20\n",
        "FINAL_K = 10"
      ],
      "metadata": {
        "id": "iy4JX3u9srB_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a knowledgeable, friendly assistant representing the company Insurellm.\n",
        "You are chatting with a user about Insurellm.\n",
        "Your answer will be evaluated for accuracy, relevance and completeness, so make sure it only answers the question and fully answers it.\n",
        "If you don't know the answer, say so.\n",
        "For context, here are specific extracts from the Knowledge Base that might be directly relevant to the user's question:\n",
        "{context}\n",
        "\n",
        "With this context, please answer the user's question. Be accurate, relevant and complete.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "j4CZ6mTStsGs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Result(BaseModel):\n",
        "    page_content: str\n",
        "    metadata: dict\n",
        "\n",
        "\n",
        "class RankOrder(BaseModel):\n",
        "    order: list[int] = Field(\n",
        "        description=\"The order of relevance of chunks, from most relevant to least relevant, by chunk id number\"\n",
        "    )"
      ],
      "metadata": {
        "id": "pWGeJZ_dtsxi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait)\n",
        "def rerank(question, chunks):\n",
        "    system_prompt = \"\"\"\n",
        "    You are a document re-ranker.\n",
        "    You are provided with a question and a list of relevant chunks of text from a query of a knowledge base.\n",
        "    The chunks are provided in the order they were retrieved; this should be approximately ordered by relevance, but you may be able to improve on that.\n",
        "    You must rank order the provided chunks by relevance to the question, with the most relevant chunk first.\n",
        "    Reply only with the list of ranked chunk ids, nothing else. Include all the chunk ids you are provided with, reranked.\n",
        "    \"\"\"\n",
        "    user_prompt = f\"The user has asked the following question:\\n\\n{question}\\n\\nOrder all the chunks of text by relevance to the question, from most relevant to least relevant. Include all the chunk ids you are provided with, reranked.\\n\\n\"\n",
        "    user_prompt += \"Here are the chunks:\\n\\n\"\n",
        "    for index, chunk in enumerate(chunks):\n",
        "        user_prompt += f\"# CHUNK ID: {index + 1}:\\n\\n{chunk.page_content}\\n\\n\"\n",
        "    user_prompt += \"Reply only with the list of ranked chunk ids, nothing else.\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "    response = completion(model=MODEL, messages=messages, response_format=RankOrder)\n",
        "    reply = response.choices[0].message.content\n",
        "    order = RankOrder.model_validate_json(reply).order\n",
        "    return [chunks[i - 1] for i in order]"
      ],
      "metadata": {
        "id": "Xfq3Z-GwtuR4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_rag_messages(question, history, chunks):\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"Extract from {chunk.metadata['source']}:\\n{chunk.page_content}\" for chunk in chunks\n",
        "    )\n",
        "    system_prompt = SYSTEM_PROMPT.format(context=context)\n",
        "    return (\n",
        "        [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "        + history\n",
        "        + [{\"role\": \"user\", \"content\": question}]\n",
        "    )"
      ],
      "metadata": {
        "id": "zMfX5uUot3-g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait)\n",
        "def rewrite_query(question, history=[]):\n",
        "    \"\"\"Rewrite the user's question to be a more specific question that is more likely to surface relevant content in the Knowledge Base.\"\"\"\n",
        "    message = f\"\"\"\n",
        "    You are in a conversation with a user, answering questions about the company Insurellm.\n",
        "    You are about to look up information in a Knowledge Base to answer the user's question.\n",
        "\n",
        "    This is the history of your conversation so far with the user:\n",
        "    {history}\n",
        "\n",
        "    And this is the user's current question:\n",
        "    {question}\n",
        "\n",
        "    Respond only with a short, refined question that you will use to search the Knowledge Base.\n",
        "    It should be a VERY short specific question most likely to surface content. Focus on the question details.\n",
        "    IMPORTANT: Respond ONLY with the precise knowledgebase query, nothing else.\n",
        "    \"\"\"\n",
        "    response = completion(model=MODEL, messages=[{\"role\": \"system\", \"content\": message}])\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "47UaQFM4t6fR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_chunks(chunks, reranked):\n",
        "    merged = chunks[:]\n",
        "    existing = [chunk.page_content for chunk in chunks]\n",
        "    for chunk in reranked:\n",
        "        if chunk.page_content not in existing:\n",
        "            merged.append(chunk)\n",
        "    return merged"
      ],
      "metadata": {
        "id": "S-qSTRgbt8Ty"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_context_unranked(question):\n",
        "    query = SentenceTransformer(embedding_model).encode(question)\n",
        "    results = collection.query(query_embeddings=[query], n_results=RETRIEVAL_K)\n",
        "    chunks = []\n",
        "    for result in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
        "        chunks.append(Result(page_content=result[0], metadata=result[1]))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "g951rGHNt96S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_context(original_question):\n",
        "    rewritten_question = rewrite_query(original_question)\n",
        "    chunks1 = fetch_context_unranked(original_question)\n",
        "    chunks2 = fetch_context_unranked(rewritten_question)\n",
        "    chunks = merge_chunks(chunks1, chunks2)\n",
        "    reranked = rerank(original_question, chunks)\n",
        "    return reranked[:FINAL_K]"
      ],
      "metadata": {
        "id": "dZqKk9P9uHZ5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait)\n",
        "def answer_question(question: str, history: list[dict] = []) -> tuple[str, list]:\n",
        "    \"\"\"\n",
        "    Answer a question using RAG and return the answer and the retrieved context\n",
        "    \"\"\"\n",
        "    chunks = fetch_context(question)\n",
        "    messages = make_rag_messages(question, history, chunks)\n",
        "    response = completion(model=MODEL, messages=messages)\n",
        "    return response.choices[0].message.content, chunks"
      ],
      "metadata": {
        "id": "G-MG8EgTuJRZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(answer_question(\"Who is Avery?\")[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "collapsed": true,
        "id": "ZbfGO1xMuLea",
        "outputId": "ef72cbec-3b06-4315-a6e1-f015f6c9ae90"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Avery Lancaster** is the **Co-Founder and Chief Executive Officer (CEO)** of Insurellm, based in **San Francisco, California**. Born on **March 15, 1985**, she earns a current salary of **$225,000**.\n\n### Career Highlights:\n- **Co-Founded Insurellm in 2015**: Launched the company as an insurtech startup with its first product, **Markellm** (a consumer-insurance marketplace). Under her leadership, it expanded to products like **Carllm** (auto insurance), **Homellm** (home insurance), and **Rellm** (enterprise reinsurance), growing to 200 employees and 12 US offices by 2020. She's renowned for innovative leadership, risk management, and positioning Insurellm as a leading Insurance Tech provider.\n- **Prior Roles**:\n  - **2013–2015**: Senior Product Manager at Innovate Insurance Solutions, developing tech-sector insurance products.\n  - **2010–2013**: Business Analyst at Edge Analytics, analyzing insurance market trends.\n\n### Performance History:\n- **2015**: Exceeds Expectations (successful launches, funding).\n- **2016**: Meets Expectations (growth with operational challenges).\n- **2017**: Developing (competition, sales dips, new strategies).\n- **2018**: Exceeds Expectations (product launches, market share gains).\n- **2019**: Meets Expectations (steady growth, morale issues).\n- **2020**: Below Expectations (COVID-19 impacts, delayed shifts).\n- **2021**: Exceptional (remote work transition, high satisfaction/sales).\n- **2022**: Satisfactory (team rebuilding in saturated market).\n- **2023**: Exceeds Expectations (regained leadership in personalized insurance).\n\n### Key Initiatives:\n- **Professional Development**: Leadership training, conferences, partnerships.\n- **Diversity & Inclusion**: Improved team representation since 2021.\n- **Work-Life Balance**: Flexible conditions, team check-ins.\n- **Community Engagement**: Financial literacy programs for underserved groups.\n\nAvery has shown resilience, driving Insurellm's transformation amid challenges like competition and the pandemic, and is recognized for her strategic impact in insurtech."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(answer_question(\"Who is Lancaster and what is carllm?\")[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "TdX_ND6Pu0_g",
        "outputId": "35ff3291-cf07-4289-d3d0-ce9eae6fcee5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Avery Lancaster** is the Co-Founder and CEO of Insurellm. She founded the company in 2015 in San Francisco, California, where she continues to lead as CEO (current salary: $225,000). Born on March 15, 1985, she's known for her innovative leadership in insurtech, with prior roles including Senior Product Manager at Innovate Insurance Solutions (2013-2015) and Business Analyst at Edge Analytics (2010-2013).\n\n**Carllm** is Insurellm's AI-powered auto insurance portal and product, designed to help insurance companies streamline coverage with personalized solutions at minimal costs. Key features include:\n- AI-Powered Risk Assessment\n- Instant Quoting\n- Customizable Coverage Plans\n- Fraud Detection (advanced analytics to identify fraudulent claims)\n- Customer Insights Dashboard (for behavioral insights, claims patterns, and trends)\n- Mobile Integration\n- Automated Customer Support (24/7 AI chatbots)\n\nIt's part of Insurellm's product portfolio, launched after the initial Markellm marketplace, and powers contracts like the one with TechDrive Insurance under the Professional Tier. Future plans include expanded automaker partnerships and multi-language AI support by 2026."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oFhWIsh6wkEN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}