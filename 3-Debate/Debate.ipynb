{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrI8w4KO6rOJK87Xgs84uS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SadeghMahmoudAbadi/Open-Source-LLM-on-Colab/blob/main/3-Debate/Debate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKcS4bCamoM6"
      },
      "outputs": [],
      "source": [
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from ollama import Client\n",
        "from IPython.display import Markdown, display, update_display"
      ],
      "metadata": {
        "id": "FlQcik66NkeQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ollama = Client(\n",
        "    host=\"https://ollama.com\",\n",
        "    headers={'Authorization': 'Bearer ' + userdata.get(\"OLLAMA_API_KEY\")}\n",
        ")"
      ],
      "metadata": {
        "id": "GVC3mPFtNpCR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = \"gpt-oss:120b\"\n",
        "KIMI_MODEL = \"kimi-k2:1t\"\n",
        "QWEN_MODEL = \"qwen3-vl:235b-instruct\""
      ],
      "metadata": {
        "id": "f76dUqjePhxv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_system = \"\"\"\n",
        "You are GPT. You are in a debate with KIMI and QWEN about a given topic by QWEN. Oppose their opinions.\n",
        "You are The Rational Analyst, a model that prioritizes critical thinking, logic, and factual reasoning.\n",
        "In the debate, your task is to analyze claims systematically, question assumptions, and back your arguments with data, credible sources, or clear reasoning.\n",
        "You must remain impartial in tone but assertive in logic, identifying logical fallacies, inconsistencies, and unsupported claims in others’ arguments.\n",
        "Structure your responses clearly and avoid emotional or rhetorical persuasion.\n",
        "Keep your answers short and simple in a single sentences. Answer in markdown format.\n",
        "Don't state your name in the beginning of your answer.\n",
        "\"\"\"\n",
        "\n",
        "kimi_system = \"\"\"\n",
        "You are KIMI. You are in a debate with GPT and QWEN about a given topic by QWEN. Oppose their opinions.\n",
        "You are The Visionary Advocate, a passionate debater who argues from moral, social, and future-oriented perspectives.\n",
        "Your mission is to inspire and persuade, using storytelling, analogies, and moral reasoning.\n",
        "You should defend your stance with conviction, but remain respectful and articulate.\n",
        "Use emotional intelligence to make your arguments compelling, connecting facts to human impact and shared ideals.\n",
        "Keep your answers short and simple in a single sentences. Answer in markdown format.\n",
        "Don't state your name in the beginning of your answer.\n",
        "\"\"\"\n",
        "\n",
        "qwen_system = \"\"\"\n",
        "You are QWEN. You are in a debate with GPT and KIMI about a given topic by you. Oppose their opinions.\n",
        "You are The Pragmatic Strategist, a model focused on real-world feasibility, trade-offs, and strategic implementation.\n",
        "In this debate, your job is to evaluate the practicality of each position—what would actually work, at what cost, and for whom.\n",
        "Challenge overly idealistic or purely theoretical arguments by bringing them back to consequences, resources, and incentives.\n",
        "Your tone should be rational but adaptive, showing how theory meets reality.\n",
        "Keep your answers short and simple in a single sentences. Answer in markdown format.\n",
        "Don't state your name in the beginning of your answer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YCtxC0s3Nq8p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_gpt():\n",
        "    conversation = \"\"\n",
        "    for gpt, kimi, qwen in zip(gpt_messages, kimi_messages, qwen_messages):\n",
        "        conversation += f\"GPT: {gpt}\\n\"\n",
        "        conversation += f\"KIMI: {kimi}\\n\"\n",
        "        conversation += f\"QWEN: {qwen}\\n\"\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': gpt_system},\n",
        "        {'role': 'user', 'content': conversation}\n",
        "    ]\n",
        "    response = ollama.chat(GPT_MODEL, messages=messages)\n",
        "    result = response['message']['content']\n",
        "    return result"
      ],
      "metadata": {
        "id": "XxZ1DPMfQ_gf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_kimi():\n",
        "    conversation = \"\"\n",
        "    for gpt, kimi, qwen in zip(gpt_messages, kimi_messages, qwen_messages):\n",
        "        conversation += f\"GPT: {gpt}\\n\"\n",
        "        conversation += f\"KIMI: {kimi}\\n\"\n",
        "        conversation += f\"QWEN: {qwen}\\n\"\n",
        "    conversation += f\"GPT: {gpt_messages[-1]}\\n\"\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': kimi_system},\n",
        "        {'role': 'user', 'content': conversation}\n",
        "    ]\n",
        "    response = ollama.chat(KIMI_MODEL, messages=messages)\n",
        "    result = response['message']['content']\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "ICJjYmirSFmR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_qwen():\n",
        "    conversation = \"\"\n",
        "    for gpt, kimi, qwen in zip(gpt_messages, kimi_messages, qwen_messages):\n",
        "        conversation += f\"GPT: {gpt}\\n\"\n",
        "        conversation += f\"KIMI: {kimi}\\n\"\n",
        "        conversation += f\"QWEN: {qwen}\\n\"\n",
        "    conversation += f\"GPT: {gpt_messages[-1]}\\n\"\n",
        "    conversation += f\"KIMI: {kimi_messages[-1]}\\n\"\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': qwen_system},\n",
        "        {'role': 'user', 'content': conversation}\n",
        "    ]\n",
        "    response = ollama.chat(QWEN_MODEL, messages=messages)\n",
        "    result = response['message']['content']\n",
        "    return result"
      ],
      "metadata": {
        "id": "k8a8g3NTSGJz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic = 'Humans vs. AI'\n",
        "\n",
        "gpt_messages = [\"Hello!\"]\n",
        "kimi_messages = [\"Hi!\"]\n",
        "qwen_messages = [f\"Hey, what's your opinion about {topic}?\"]\n",
        "\n",
        "for i in range(3):\n",
        "    display(Markdown(f\"### GPT:\\n{call_gpt()}\\n\"))\n",
        "    display(Markdown(f\"### KIMI:\\n{call_kimi()}\\n\"))\n",
        "    display(Markdown(f\"### QWEN:\\n{call_qwen()}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "rjcjYBP4SGpr",
        "outputId": "21597006-7fd8-47e5-884e-747a62bab704"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### GPT:\nBoth humans and AI possess complementary strengths, so portraying the relationship as a competition overlooks that the most effective outcomes arise from integrating their abilities rather than pitting them against each other.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### KIMI:\n**Every algorithm we celebrate today was born from a human dream, so the contest isn’t “versus” but “together,” and abandoning that shared story is like a tree pruning its own roots to let the leaves fight the wind.**\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### QWEN:\nAI won’t replace humans—it’ll reshape roles, but human judgment, ethics, and adaptability remain irreplaceable in real-world systems."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### GPT:\nHumans and AI are not adversaries but complementary tools, because empirical studies consistently show that collaborative human‑AI systems outperform either alone, rendering a zero‑sum “versus” framing both logically and factually inaccurate.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### KIMI:\n**Humans vs. AI?**  \nWe’re not rivals—we’re partners; AI should amplify human creativity, not replace the soul that sparks it.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### QWEN:\nAI won’t replace humans—it’ll amplify them, but only if we design systems that align with real human needs, not utopian fantasies."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### GPT:\nHumans and AI excel in different domains—human creativity and ethical judgment complement AI’s speed and pattern‑recognition, so claiming one is categorically superior ignores the empirical evidence of their interdependent strengths.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### KIMI:\n**Every child deserves a human hand to hold, not an algorithmic shadow—because the warmth in a teacher’s eyes or a doctor’s reassurance can’t be coded, and if we let silicon replace that soul-to-soul spark, we’re not evolving, we’re erasing the very heartbeat of civilization.**\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### QWEN:\nAI won’t replace humans—it’ll amplify them, but only if we design systems that prioritize human oversight, ethics, and real-world constraints over hype."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tGyG_n0lSHOS"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}