{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYUrcGFyFLHZPssT11YR1J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SadeghMahmoudAbadi/Open-Source-LLM-on-Colab/blob/main/5-Tokenizers/Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "ZknDhk1SSUXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gc"
      ],
      "metadata": {
        "id": "POWxCNS6xDs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "s4oM0_ljxFcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PHI = \"microsoft/Phi-4-mini-instruct\"\n",
        "QWEN = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "DEEPSEEK = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "GEMMA = \"google/gemma-3-270m-it\""
      ],
      "metadata": {
        "id": "bqXyK4HdxG7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a fun fact.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "SVO1_FaaxQMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "I22BbbfgxVjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, messages, quant=True, max_new_tokens=500):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "    attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=\"cuda\")\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "    if quant:\n",
        "        model = AutoModelForCausalLM.from_pretrained(model, quantization_config=quant_config).to(\"cuda\")\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(model).to(\"cuda\")\n",
        "    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, streamer=streamer)\n",
        "\n",
        "    # Clean up memory\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Aco4iTGkxYXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(PHI, messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "327882e2a0ee4ecebbeb3aa347a99231",
            "18c1e4041ff948b3a00a0ff1fe731997",
            "f12cc7a295d2455588b47bbc91215d10",
            "87c13cb4362e4836bbd760644ab894ad",
            "4d0285bfa787475085eb372c6568bcd7",
            "151758d1e5534da196e2120c5f211b94",
            "fbc1c35878114284a680d5fcb3b60bac",
            "f4383c433e664324945394cd0dc3589a",
            "f4dc87a6a8584d3580803d4c7ef3e003",
            "653cafab9f1247bcb61069686eb8f031",
            "f5d8cbbf7f9c4a19bf4ee4ce9b04831e"
          ]
        },
        "id": "lSIoubjwyJeR",
        "outputId": "f8d84859-e1be-4c58-c8a3-1f90fd415937"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "327882e2a0ee4ecebbeb3aa347a99231",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|user|>Tell me a fun fact.<|end|><|assistant|>Did you know that octopuses have three hearts? Two pump blood to the gills, while the third pumps it to the rest of the body. Additionally, octopuses have blue blood, which contains copper-based molecules that bind to oxygen. This adaptation allows them to survive in low-oxygen environments. Isn't that fascinating?<|end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(QWEN, messages)"
      ],
      "metadata": {
        "id": "hoWPVRa7yPEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(DEEPSEEK, messages, quant=False)"
      ],
      "metadata": {
        "id": "P6MpbL0Ky4GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(GEMMA, messages, quant=False)"
      ],
      "metadata": {
        "id": "mMlfrhsB2LEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(QWEN, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "_RsW5HXe4PUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"My name is Sadegh Mahmoud Abadi, and I'm eager about LLMs!\"\n",
        "tokens = tokenizer.encode(text)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj3Z8CQD6pBk",
        "outputId": "d0dacbd9-e32e-4212-e463-79c1598ef893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5050,\n",
              " 829,\n",
              " 374,\n",
              " 30681,\n",
              " 791,\n",
              " 71,\n",
              " 93186,\n",
              " 3680,\n",
              " 2767,\n",
              " 11,\n",
              " 323,\n",
              " 358,\n",
              " 2776,\n",
              " 23541,\n",
              " 911,\n",
              " 444,\n",
              " 10994,\n",
              " 82,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "character_count = len(text)\n",
        "word_count = len(text.split(' '))\n",
        "token_count = len(tokens)\n",
        "print(f\"There are {character_count} characters, {word_count} words and {token_count} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmfWRj0U69tG",
        "outputId": "651b415e-5dc7-4eea-f97f-3c7de49f9b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 58 characters, 11 words and 19 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZmkTZeJF7CVa",
        "outputId": "0cd20c68-b05c-4923-fa59-323cb91ecfbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"My name is Sadegh Mahmoud Abadi, and I'm eager about LLMs!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0b0L1B37ELr",
        "outputId": "61055f46-d333-4e59-e650-e8feb79e11ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My',\n",
              " ' name',\n",
              " ' is',\n",
              " ' Sad',\n",
              " 'eg',\n",
              " 'h',\n",
              " ' Mahmoud',\n",
              " ' Ab',\n",
              " 'adi',\n",
              " ',',\n",
              " ' and',\n",
              " ' I',\n",
              " \"'m\",\n",
              " ' eager',\n",
              " ' about',\n",
              " ' L',\n",
              " 'LM',\n",
              " 's',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z_cKP307GPP",
        "outputId": "cf79a0e4-ae92-4099-a17c-f0db8311ddf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>user\n",
            "Tell me a fun fact.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Ilx4ArX7WL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
